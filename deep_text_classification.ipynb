{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@author : https://github.com/nitish11  \n",
    "#@decription : finding relationship betwen two sentences \n",
    "#@date : 31st June, 2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective : \n",
    "\n",
    "* Text classification for given two columns of data *train_data[[1,2]]*.\n",
    "* No. of classes = 5 *[' ForwardEntailment', ' Independent', ' Equivalence', ' ReverseEntailment', ' OtherRelated']*\n",
    "\n",
    "* Other columns contain extra  information about the two segments of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach \n",
    "\n",
    "### Using classical method of classification\n",
    "\n",
    "* Preprcoessing of columns of the given data\n",
    "    * Extract the numeral information from the data given as extra columns\n",
    "    * Store only these columns as training data\n",
    "* Use the extracted information from training data and perform classical methods (Random Forest, XGboost) for classification. \n",
    "\n",
    "\n",
    "### Deep Learning Solution \n",
    "\n",
    "* Use only starting 2 columns (text_data) and do pre-processing.\n",
    "* Use word2vec or Glove for representing text as vectors.\n",
    "* Use the vectors and do classiication using differen models as LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('-- keys : ', Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], dtype='int64'))\n",
      "('---text_data:',                  1                     2\n",
      "0   used to treat              to treat \n",
      "1    education is    education programs \n",
      "2    are reviewed      are under review )\n",
      "('--- Results :', array([' ForwardEntailment', ' Independent', ' Equivalence',\n",
      "       ' ReverseEntailment', ' OtherRelated'], dtype=object))\n",
      "('--- #of rows:', 1000)\n"
     ]
    }
   ],
   "source": [
    "#Loading training data\n",
    "ppdb_train_data = pd.read_csv(\"ppdb.train.csv\", header=None, sep=',')\n",
    "\n",
    "print(\"-- keys : \",ppdb_train_data.keys())\n",
    "print(\"---text_data:\",ppdb_train_data[[1,2]].head(3))\n",
    "print(\"--- Results :\",ppdb_train_data[17].unique())\n",
    "print(\"--- #of rows:\",len(ppdb_train_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "0 : [VP/NP] \n",
      "1 :  used to treat \n",
      "2 :  to treat \n",
      "3 :  PPDB2.0Score=4.76476 PPDB1.0Score=12.744950 -logp(LHS\n",
      "4 : e1)=0.30786 -logp(LHS\n",
      "5 : e2)=0.89456 -logp(e1\n",
      "6 : LHS)=14.95995 -logp(e1\n",
      "7 : e2)=7.90234 -logp(e1\n",
      "8 : e2,LHS)=6.75647 -logp(e2\n",
      "9 : LHS)=12.48692 -logp(e2\n",
      "10 : e1)=4.84261 -logp(e2\n",
      "11 : e1,LHS)=4.28343 AGigaSim=0.80561 Abstract=0 Adjacent=0 CharCountDiff=-5 CharLogCR=-0.48551 ContainsX=0 Equivalence=0.088485 Exclusion=0.047945 GlueRule=0 GoogleNgramSim=0.33742 Identity=0 Independent=0.133904 Lex(e1\n",
      "12 : e2)=63.30685 Lex(e2\n",
      "13 : e1)=63.30685 Lexical=1 LogCount=0 MVLSASim=NA Monotonic=1 OtherRelated=0.018057 PhrasePenalty=1 RarityPenalty=0.01832 ForwardEntailment=0.711609 SourceTerminalsButNoTarget=0 SourceWords=3 TargetComplexity=0.97133 TargetTerminalsButNoSource=0 TargetWords=2 UnalignedSource=0 UnalignedTarget=0 WordCountDiff=-1 WordLenDiff=-0.16667 WordLogCR=-0.40547 \n",
      "14 : nan\n",
      "15 : nan\n",
      "16 :  0-0 1-0 2-1 \n",
      "17 :  ForwardEntailment\n"
     ]
    }
   ],
   "source": [
    "#Displaying one data-point \n",
    "print \"-\"*80\n",
    "for key in ppdb_train_data.columns:\n",
    "    print key,\":\", ppdb_train_data.iloc[0][key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusing data points \n",
    "\n",
    "#### Reference : the above columns \n",
    "\n",
    "* Columns 4th, 5th, 6th, 7th, 8th, 9th and 10th are very confusing and not making sense.\n",
    "* I think there is an issue in making this csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_data preparation \n",
    "columns = [\"text1\", \"text2\", \"result\"]\n",
    "\n",
    "#Creating blank dataframe\n",
    "dummy_data = np.random.randn(len(ppdb_train_data.index),len(columns))\n",
    "train_data = pd.DataFrame(dummy_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.871196</td>\n",
       "      <td>0.752788</td>\n",
       "      <td>-0.172124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.550445</td>\n",
       "      <td>-0.113680</td>\n",
       "      <td>0.701426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text1     text2    result\n",
       "0  1.871196  0.752788 -0.172124\n",
       "1 -0.550445 -0.113680  0.701426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Unique Results :  [' ForwardEntailment' ' Independent' ' Equivalence' ' ReverseEntailment'\n",
      " ' OtherRelated']\n",
      " Equivalence          303\n",
      " ReverseEntailment    301\n",
      " Independent          212\n",
      " ForwardEntailment    176\n",
      " OtherRelated           8\n",
      "Name: 17, dtype: int64\n",
      "-- Updated Results :  [ 0.  1.  2.  3.  4.]\n",
      "2.0    303\n",
      "3.0    301\n",
      "1.0    212\n",
      "0.0    176\n",
      "4.0      8\n",
      "Name: result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of result column\n",
    "print \"-- Unique Results : \",ppdb_train_data[17].unique()\n",
    "print pd.value_counts(ppdb_train_data[17])\n",
    "\n",
    "for index,element in enumerate(ppdb_train_data[17].unique()):\n",
    "    indices = ppdb_train_data[ppdb_train_data[17] == element].index.tolist()\n",
    "    train_data.loc[indices, \"result\"] = index\n",
    "\n",
    "print \"-- Updated Results : \",train_data[\"result\"].unique()\n",
    "print pd.value_counts(train_data[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Processing of text column\n",
    "train_data[\"text1\"] = ppdb_train_data[1]\n",
    "train_data[\"text2\"] = ppdb_train_data[2]\n",
    "\n",
    "#Adding the two columns of text\n",
    "train_data[\"text\"] = train_data[\"text1\"]+' '+train_data[\"text2\"]  \n",
    "train_data[\"result\"] = train_data[\"result\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>result</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used to treat</td>\n",
       "      <td>to treat</td>\n",
       "      <td>0</td>\n",
       "      <td>used to treat   to treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>education is</td>\n",
       "      <td>education programs</td>\n",
       "      <td>1</td>\n",
       "      <td>education is   education programs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text1                 text2  result  \\\n",
       "0   used to treat              to treat        0   \n",
       "1    education is    education programs        1   \n",
       "\n",
       "                                  text  \n",
       "0            used to treat   to treat   \n",
       "1   education is   education programs   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example of Estimator for DNN-based text classification with DBpedia data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import encoders\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "n_words = 0\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(features, target):\n",
    "    global num_classes\n",
    "    \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(target, num_classes, 1, 0)\n",
    "    logits = tf.contrib.layers.fully_connected(encoding, num_classes, activation_fn=None)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.01)\n",
    "\n",
    "    return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "    }, loss, train_op)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used to treat   to treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>education is   education programs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>are reviewed   are under review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeah , it 's   yes , it is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is no doubt   is no dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it is essential for   it is necessary for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>approved for sale   approved for use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rights and liberties of   rights and freedoms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s. alber delivered his opinion   alber delive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>these exist   they are available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of individuals belonging   of people belonging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>educational institutions ,   education instit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>still is   to continue to be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>- yes , it does   yeah , he did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>for strengthening   with a view to further st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>government departments or agencies   departme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>are actually   is truly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>were reluctant to   were unwilling to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>you look really pretty   you 're a very prett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>covenant and is   covenant , as well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>following completion   following the conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>report of the secretary-general entitled   re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a million dollars   dollars ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>but nevertheless   there are , however</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>strengthen its efforts   enhance its efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>be participating in   engage in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ministers '   , foreign ministers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>, there were about   there were around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>aligns itself with the statement   has aligne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>board recommended that   board recommends tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>all appropriate measures to   relevant measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>the security council should   the council sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>about one million   approximately one million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>although there were   while there has been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>participated actively   actively participate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>must be identified   must be determined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>and other stakeholders to   and other relevan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>consultations during the   consultations betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>was recognized   well recognized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>are essential to the   is critical for the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>, government departments and   and department...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>and is then   and subsequently by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>advantage of this   the benefits of this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>communications technologies   of communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>shall apply , mutatis   are applicable mutatis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>the tribunal is   the court is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>have become increasingly   are becoming more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>been introduced into   been incorporated into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>- no , she 's   no , she is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>in the region   of the states of the region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>cooperation between the united nations   part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>communications and consultation   communicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>relating to an   relating to the status of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>is important for   is crucial to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>designed to strengthen   designed to enhance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>programmes and services   programs , and serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>i 'm doing good   - i 'm all right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>enhance the legitimacy   increase the legitim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>tell me the truth   'm telling you the truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>'re going home !   'm going home .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                            used to treat   to treat \n",
       "1                   education is   education programs \n",
       "2                     are reviewed   are under review \n",
       "3                          yeah , it 's   yes , it is \n",
       "4                         is no doubt   is no dispute \n",
       "5           it is essential for   it is necessary for \n",
       "6                approved for sale   approved for use \n",
       "7     rights and liberties of   rights and freedoms...\n",
       "8     s. alber delivered his opinion   alber delive...\n",
       "9                    these exist   they are available \n",
       "10     of individuals belonging   of people belonging \n",
       "11    educational institutions ,   education instit...\n",
       "12                       still is   to continue to be \n",
       "13                    - yes , it does   yeah , he did \n",
       "14    for strengthening   with a view to further st...\n",
       "15    government departments or agencies   departme...\n",
       "16                            are actually   is truly \n",
       "17              were reluctant to   were unwilling to \n",
       "18    you look really pretty   you 're a very prett...\n",
       "19               covenant and is   covenant , as well \n",
       "20    following completion   following the conclusion \n",
       "21    report of the secretary-general entitled   re...\n",
       "22                      a million dollars   dollars , \n",
       "23             but nevertheless   there are , however \n",
       "24       strengthen its efforts   enhance its efforts \n",
       "25                    be participating in   engage in \n",
       "26                  ministers '   , foreign ministers \n",
       "27             , there were about   there were around \n",
       "28    aligns itself with the statement   has aligne...\n",
       "29    board recommended that   board recommends tha...\n",
       "..                                                 ...\n",
       "970   all appropriate measures to   relevant measur...\n",
       "971   the security council should   the council sho...\n",
       "972     about one million   approximately one million \n",
       "973        although there were   while there has been \n",
       "974      participated actively   actively participate \n",
       "975           must be identified   must be determined \n",
       "976   and other stakeholders to   and other relevan...\n",
       "977   consultations during the   consultations betw...\n",
       "978                  was recognized   well recognized \n",
       "979        are essential to the   is critical for the \n",
       "980   , government departments and   and department...\n",
       "981                 and is then   and subsequently by \n",
       "982          advantage of this   the benefits of this \n",
       "983    communications technologies   of communication \n",
       "984    shall apply , mutatis   are applicable mutatis \n",
       "985                    the tribunal is   the court is \n",
       "986   have become increasingly   are becoming more ...\n",
       "987     been introduced into   been incorporated into \n",
       "988                       - no , she 's   no , she is \n",
       "989       in the region   of the states of the region \n",
       "990   cooperation between the united nations   part...\n",
       "991   communications and consultation   communicati...\n",
       "992        relating to an   relating to the status of \n",
       "993                  is important for   is crucial to \n",
       "994      designed to strengthen   designed to enhance \n",
       "995   programmes and services   programs , and serv...\n",
       "996                i 'm doing good   - i 'm all right \n",
       "997   enhance the legitimacy   increase the legitim...\n",
       "998      tell me the truth   'm telling you the truth \n",
       "999                're going home !   'm going home . \n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(unused_argv):\n",
    "    global n_words, train_data\n",
    "    # Prepare training and testing data\n",
    "    # x_train =  pd.DataFrame(train_data[[\"text1\",\"text2\"]])\n",
    "    x_train = pd.DataFrame(train_data[\"text\"])\n",
    "    y_train = pd.Series(train_data[\"result\"].values)\n",
    "    x_test =  pd.DataFrame(train_data[\"text\"].head(10))\n",
    "    y_test =  pd.Series(train_data[\"result\"].values[:10])\n",
    "    \n",
    "    \n",
    "    # Process vocabulary\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "\n",
    "    x_transform_train = vocab_processor.fit_transform(x_train)\n",
    "    x_transform_test = vocab_processor.transform(x_test)\n",
    "\n",
    "    x_train = np.array(list(x_transform_train))\n",
    "    x_test = np.array(list(x_transform_test))\n",
    "\n",
    "    n_words = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % n_words)\n",
    "\n",
    "    # Build model\n",
    "    model_fn = rnn_model\n",
    "    classifier = learn.Estimator(model_fn=model_fn)\n",
    "\n",
    "    # Train and predict\n",
    "    classifier.fit(x_train, y_train, steps=100)\n",
    "    print(\"----------------classifier.predict(x_test)\\n\")\n",
    "    for p in classifier.predict(x_test):\n",
    "        print(\"---------------------\",p)\n",
    "    \n",
    "    y_predicted = [ p['class'] for p in classifier.predict(x_test) ]\n",
    "    \n",
    "    print(\"-- x_train     \",np.shape(x_train))\n",
    "    print(\"-- x_test     \",np.shape(x_test))\n",
    "    \n",
    "    print(\"-- y_train     \",np.shape(y_train))\n",
    "    print(\"-- y_test     \",np.shape(y_test))\n",
    "    print(\"-- y_predicted\",np.shape(y_predicted))\n",
    "    \n",
    "    print(\"-- y_test     \",y_test)\n",
    "    print(\"-- y_predicted\",y_predicted)\n",
    "    \n",
    "    score = metrics.accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpQ0AMwc\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f716032e290>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:From <ipython-input-18-15de238d36fb>:28: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-18-15de238d36fb>:28: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-16-bb6094e459af>:27: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    }
   ],
   "source": [
    "tf.app.run(main=classify, argv=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
